本文档用来介绍项目中使用到的AI服务器

## Ollama 简介

Ollama 是一个开源平台，旨在简化 AI 模型的本地运行。它支持多种模型和任务，用户可以通过命令行接口或者 API 与模型进行交互。Ollama 提供了一些开箱即用的功能，使得用户能够快速部署并使用 AI 模型进行各种任务。

### 主要特点

1. **支持多种语言模型**：
   
   - Ollama 支持多种自然语言处理（NLP）任务，用户可以通过 Ollama 接口访问多个预训练的语言模型。

2. **本地部署**：
   
   - 用户可以在本地计算机上部署 Ollama，无需依赖外部服务，从而更好地控制数据隐私和模型训练过程。

3. **易于集成**：
   
   - 提供简洁的命令行工具和 API，使得开发者能够快速集成到自己的项目中。

4. **开放源代码**：
   
   - Ollama 是一个开源项目，任何人都可以自由查看、修改和贡献代码。

5. **高效性**：
   
   - 采用高效的推理框架，Ollama 可以快速处理模型推理请求，适用于多种应用场景。

### 安装与使用

#### 安装

在Windows上，你可以通过以下命令在本地下载 Ollama 安装包：

```bash
# 安装 Ollama
curl -O https://ollama.com/download/OllamaSetup.exe
```

如果你是LinUX，你可以使用下面的命令：

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

ollama的默认安装路径是C盘，如果你需要安装的其他盘，需要手动剪切安装好的文件，并修改环境变量。

#### 使用

安装完成后，你可以使用命令行工具与模型进行交互。例如，调用一个模型：

```bash
ollama run model
```

这个命令会先检索你是否有对应的本地模型，如果没有会自动下载（前提是[ollama](https://ollama.com/search)平台得有)

### 配置

在使用ollama的过程中，你可能想将它配置到其他软件或者服务器里，但是首先，你需要设置一些环境变量：

| 名                       | 描述                | 默认值      |
| ----------------------- | ----------------- | -------- |
| `CUDA_VISIBLE_[device]` | 设置启用的GPU序号        | 0（即无GPU） |
| `OLLAMA_HOST`           | ollama服务器端口       | :8080    |
| `OLLAMA_MODELS`         | ollama 下载到的模型存放路径 | `True`   |
| `OLLAMA_ORIGINS`        |                   | `*`      |

### 支持的任务

Ollama 支持以下几种常见的 NLP 任务：

- **文本生成**：生成流畅的、符合上下文的文本。
- **文本总结**：对输入的长文本进行摘要，提取关键信息。
- **情感分析**：分析文本的情感，判断文本是积极、消极还是中性。
- **对话生成**：与用户进行自然对话，提供合适的回答。
- **命名实体识别**：识别文本中的重要实体，如人名、地名、组织名等。

### 适用场景

Ollama 可以应用于以下场景：

- **聊天机器人**：通过语言模型与用户进行互动。
- **文本自动化处理**：进行自动化的文本生成、摘要和分析任务。
- **教育与学习**：为学生和开发者提供自然语言处理的工具，帮助他们理解和应用 AI 技术。
- **内容创作**：生成文章、小说、博客等内容。

## 总结

Ollama 是一个高效、易用且灵活的 AI 生成平台，适用于多种语言处理任务。它支持本地部署，开源且易于集成，能够帮助开发者快速实现基于 AI 的应用。通过简单的命令行操作和 API，Ollama 使得开发者可以轻松调用预训练的模型进行各种任务处理。
